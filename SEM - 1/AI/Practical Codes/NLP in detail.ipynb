{"cells":[{"cell_type":"code","execution_count":null,"id":"d91fe5b3","metadata":{"id":"d91fe5b3"},"outputs":[],"source":["#Tokenization#"]},{"cell_type":"code","execution_count":null,"id":"1d225376","metadata":{"id":"1d225376","outputId":"4a67a70b-33ae-4009-d2b8-f4b6f0b892a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["['This', 'is', 'an', 'example', 'of', 'tokenization', 'in', 'NLP', 'tasks', '.']\n"]}],"source":["import nltk\n","\n","# Download the punkt package\n","nltk.download('punkt')\n","\n","# take a sample text for generating tokens\n","text = \"This is an example of tokenization in NLP tasks.\"\n","tokens = nltk.word_tokenize(text)\n","print(tokens)"]},{"cell_type":"code","execution_count":null,"id":"ad00d4c1","metadata":{"id":"ad00d4c1"},"outputs":[],"source":["###Stemming"]},{"cell_type":"code","execution_count":null,"id":"ecf726e2","metadata":{"id":"ecf726e2","outputId":"7e24be3b-a874-44fc-9693-ee090f92bebd"},"outputs":[{"name":"stdout","output_type":"stream","text":["['run', 'runner', 'ran', 'run', 'easili', 'fairli']\n"]}],"source":["import nltk\n","from nltk.stem import PorterStemmer\n","\n","# create stemmer object using PorterStemmer method of nltk library\n","stemmer = PorterStemmer()\n","words = [\"run\", \"runner\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n","stemmed_words = [stemmer.stem(word) for word in words]\n","print(stemmed_words)"]},{"cell_type":"code","execution_count":null,"id":"a7ea1c5c","metadata":{"id":"a7ea1c5c","outputId":"a1531487-a3dd-4c47-e885-a62287f5d199"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\Radhe\\AppData\\Roaming\\nltk_data...\n"]},{"name":"stdout","output_type":"stream","text":["run\n"]}],"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download the wordnet package\n","nltk.download('wordnet')\n","\n","# Define the word to be lemmatized\n","word = \"running\"\n","\n","# Create an instance of the WordNetLemmatizer# Lemmatize the word\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","lemma = lemmatizer.lemmatize(word, pos='v')\n","print(lemma)"]},{"cell_type":"code","execution_count":null,"id":"48c390e0","metadata":{"id":"48c390e0","outputId":"12179b78-16c4-4a14-a9a6-314384f49958"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'.', 'example', 'building', 'of', 'vocabulary', 'for', 'a', 'NLP', 'is', 'This', 'tasks', 'an'}\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Tokenize the text\n","text = \"This is an example of building a vocabulary for NLP tasks.\"\n","words = word_tokenize(text)\n","\n","# Create a vocabulary\n","vocab = set(words)\n","print(vocab)"]},{"cell_type":"code","execution_count":null,"id":"9d2defd9","metadata":{"id":"9d2defd9","outputId":"9ffbe5af-f10f-4021-867b-f1513c31479f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('This', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'of'), ('of', 'creating'), ('creating', 'n-grams'), ('n-grams', '.')]\n","[('This', 'is', 'an'), ('is', 'an', 'example'), ('an', 'example', 'of'), ('example', 'of', 'creating'), ('of', 'creating', 'n-grams'), ('creating', 'n-grams', '.')]\n"]}],"source":["import nltk\n","from nltk.util import ngrams\n","\n","# Tokenize the text\n","text = \"This is an example of creating n-grams.\"\n","words = nltk.word_tokenize(text)\n","\n","# Create bigrams\n","bigrams = ngrams(words, 2)\n","print(list(bigrams))\n","\n","# Create trigrams\n","trigrams = ngrams(words, 3)\n","print(list(trigrams))"]},{"cell_type":"code","execution_count":null,"id":"37356e82","metadata":{"id":"37356e82","outputId":"91c1f566-c0f2-4611-824d-ad3c2c640234"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 1 0 1 1 1]\n"," [0 1 1 0 1 1]\n"," [1 0 0 1 1 0]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Define the text data\n","text_data = [\"This is a positive sentence.\",\n","             \"This is a negative sentence.\",\n","             \"Another positive sentence.\"]\n","\n","# Create an instance of the CountVectorizer\n","vectorizer = CountVectorizer()\n","\n","# Fit the vectorizer on the text data\n","vectorizer.fit(text_data)\n","\n","# Transform the text data into numerical vectors\n","vectors = vectorizer.transform(text_data)\n","print(vectors.toarray())"]},{"cell_type":"code","execution_count":null,"id":"325f4403","metadata":{"id":"325f4403","outputId":"f072573f-54de-4580-ccc4-f18fa23e418f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[11]\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","\n","data = fetch_20newsgroups()\n","\n","model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n","model.fit(data.data, data.target)\n","\n","labels = model.predict([\"This is some example text\"])\n","print(labels)"]},{"cell_type":"code","execution_count":null,"id":"f3d16abb","metadata":{"id":"f3d16abb","outputId":"688bcc8a-fa65-4c4a-8320-2b54e3ad0ca0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in c:\\users\\radhe\\anaconda3\\lib\\site-packages (23.3.2)\n","Requirement already satisfied: setuptools in c:\\users\\radhe\\anaconda3\\lib\\site-packages (69.0.3)\n","Requirement already satisfied: wheel in c:\\users\\radhe\\anaconda3\\lib\\site-packages (0.42.0)\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]}],"source":["!pip install pip setuptools wheel"]},{"cell_type":"markdown","id":"305b8a59","metadata":{"id":"305b8a59"},"source":["In this example, we use 20newsgroups data, which contains nearly 19,000 newsgroup documents, partitioned nearly evenly across 20 different newsgroups. The pipeline vectorizes the text using TfidfVectorizer, then trains a Naive Bayes classifier using the vectorized data and returns the predicted label."]},{"cell_type":"code","execution_count":null,"id":"fdb22c29","metadata":{"id":"fdb22c29","outputId":"a233f60d-451c-4125-ef7d-9f1e0e3e3523"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: spacy in c:\\users\\radhe\\anaconda3\\lib\\site-packages (3.2.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n","Requirement already satisfied: pathy>=0.3.5 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n","Requirement already satisfied: numpy>=1.15.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n","Requirement already satisfied: jinja2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n","Requirement already satisfied: setuptools in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (69.0.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n","Requirement already satisfied: colorama in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"]},{"name":"stderr","output_type":"stream","text":["DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]}],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":null,"id":"e4f2893d","metadata":{"id":"e4f2893d","outputId":"8b9b2163-f846-4613-d6c1-946ffa8219ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.2.0"]},{"name":"stderr","output_type":"stream","text":["2023-12-28 10:56:34.950117: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n","2023-12-28 10:56:34.950167: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","DEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\n","DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n","     ---------------------------------------- 13.9/13.9 MB 2.0 MB/s eta 0:00:00\n","Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n","Requirement already satisfied: pathy>=0.3.5 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n","Requirement already satisfied: numpy>=1.15.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: jinja2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: setuptools in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (69.0.3)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.2)\n","Requirement already satisfied: colorama in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\radhe\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n","Installing collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.2.0\n","[+] Download and installation successful\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"id":"ee711e18","metadata":{"id":"ee711e18","outputId":"af423660-0f02-4016-ba9f-d57b2634d8a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Apple ORG\n","U.K. GPE\n","$1 billion MONEY\n"]}],"source":["import spacy\n","\n","# Load the spaCy model for named entity recognition\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Define a sample text\n","text = \"Apple is looking at buying U.K. startup for $1 billion\"\n","\n","# Process the text with spaCy\n","doc = nlp(text)\n","\n","# Iterate over the entities in the text and print them\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"]},{"cell_type":"code","execution_count":null,"id":"641f474c","metadata":{"id":"641f474c","outputId":"f552c545-9f87-4cd4-ae7d-49007ddcd2c2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\Radhe\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"code","execution_count":null,"id":"46ec24e9","metadata":{"id":"46ec24e9","outputId":"76a67a0b-86cd-4af0-91b9-43c90e37174f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"]}],"source":["# Download the punkt package\n","nltk.download('punkt')\n","\n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Tokenize the text into words\n","tokens = nltk.word_tokenize(text)\n","\n","# Perform part-of-speech tagging on the tokenized words\n","tagged_words = nltk.pos_tag(tokens)\n","\n","print(tagged_words)"]},{"cell_type":"code","execution_count":null,"id":"b57ff336","metadata":{"id":"b57ff336","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766919020171,"user_tz":-330,"elapsed":1114,"user":{"displayName":"Aastha Thakker","userId":"05632659073420520532"}},"outputId":"c6430f30-b2d0-4e62-b608-1b0468b2249c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Original Text:\n"," Natural Language Processing is the foundation of modern AI. ChatGPT understands text amazingly well!\n","\n","Tokens: ['Natural', 'Language', 'Processing', 'is', 'the', 'foundation', 'of', 'modern', 'AI', '.', 'ChatGPT', 'understands', 'text', 'amazingly', 'well', '!']\n"]}],"source":["#This code snippet demonstrates basic Natural Language Processing (NLP) tasks using the NLTK library in Python. It first imports nltk and downloads the punkt_tab tokenizer, which is necessary\n","# for word tokenization. Then, it defines a sample text string. After that, it uses word_tokenize from nltk.tokenize to break down the text into individual tokens (words and punctuation), and\n","# finally prints both the original text and the resulting tokens.\n","import nltk\n","nltk.download('punkt_tab')\n","text = \"Natural Language Processing is the foundation of modern AI. ChatGPT understands text amazingly well!\"\n","print(\"Original Text:\\n\", text)\n","# -----------------------------\n","# 2. Tokenization\n","# -----------------------------\n","from nltk.tokenize import word_tokenize\n","tokens = word_tokenize(text)\n","print(\"\\nTokens:\", tokens)\n"]},{"cell_type":"code","source":["# Basic NLP code in one go\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","\n","# Download resources (run once)\n","nltk.download('maxent_ne_chunker_tab')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('punkt_tab')\n","\n","# Sample text\n","text = \"Apple is looking at buying a U.K. startup for $1 billion. Natural Language Processing is amazing!\"\n","\n","# 1. Sentence Tokenization\n","print(\"Sentence Tokenization:\")\n","print(sent_tokenize(text))\n","\n","# 2. Word Tokenization\n","print(\"\\nWord Tokenization:\")\n","words = word_tokenize(text)\n","print(words)\n","\n","# 3. Stopword Removal\n","stop_words = set(stopwords.words('english'))\n","filtered = [w for w in words if w.lower() not in stop_words and w.isalpha()]\n","print(\"\\nAfter Stopword Removal:\")\n","print(filtered)\n","\n","# 4. Stemming\n","ps = PorterStemmer()\n","stemmed = [ps.stem(w) for w in filtered]\n","print(\"\\nStemming:\")\n","print(stemmed)\n","\n","# 5. Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = [lemmatizer.lemmatize(w) for w in filtered]\n","print(\"\\nLemmatization:\")\n","print(lemmatized)\n","\n","# 6. Part-of-Speech (POS) Tagging\n","print(\"\\nPOS Tagging:\")\n","print(nltk.pos_tag(filtered))\n","\n","# 7. Named Entity Recognition (NER)\n","print(\"\\nNamed Entity Recognition:\")\n","print(nltk.ne_chunk(nltk.pos_tag(filtered)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1E_V-YxmJoP","executionInfo":{"status":"ok","timestamp":1766919301922,"user_tz":-330,"elapsed":10829,"user":{"displayName":"Aastha Thakker","userId":"05632659073420520532"}},"outputId":"0933b8ed-ab9d-46c0-ff09-61c3a122d037"},"id":"d1E_V-YxmJoP","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package maxent_ne_chunker_tab to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Sentence Tokenization:\n","['Apple is looking at buying a U.K. startup for $1 billion.', 'Natural Language Processing is amazing!']\n","\n","Word Tokenization:\n","['Apple', 'is', 'looking', 'at', 'buying', 'a', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'Natural', 'Language', 'Processing', 'is', 'amazing', '!']\n","\n","After Stopword Removal:\n","['Apple', 'looking', 'buying', 'startup', 'billion', 'Natural', 'Language', 'Processing', 'amazing']\n","\n","Stemming:\n","['appl', 'look', 'buy', 'startup', 'billion', 'natur', 'languag', 'process', 'amaz']\n","\n","Lemmatization:\n","['Apple', 'looking', 'buying', 'startup', 'billion', 'Natural', 'Language', 'Processing', 'amazing']\n","\n","POS Tagging:\n","[('Apple', 'NNP'), ('looking', 'VBG'), ('buying', 'VBG'), ('startup', 'NN'), ('billion', 'CD'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('amazing', 'NN')]\n","\n","Named Entity Recognition:\n","(S\n","  Apple/NNP\n","  looking/VBG\n","  buying/VBG\n","  startup/NN\n","  billion/CD\n","  Natural/NNP\n","  Language/NNP\n","  Processing/NNP\n","  amazing/NN)\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"1Pj1X-Ez9oBwnQDH4rt8sTwoa7pT3YWa2","timestamp":1766918901797}]}},"nbformat":4,"nbformat_minor":5}